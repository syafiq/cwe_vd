{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934f39bf-3b24-435b-8891-676b9f4a26ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import sqlite3 as lite\n",
    "import hashlib\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcdb67b-9fe6-4ca3-b6e2-c8945c682cb4",
   "metadata": {},
   "source": [
    "# DiverseVul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86bc2348-ef87-456c-8ed8-5fad31f989be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742990ac96374c4e884fdbc20fc1e928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_diversevul = duckdb.query('''SELECT * FROM read_json('../../datasets/diversevul_dataset.json', auto_detect=True)''').to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "781e1842-2a96-4562-b683-3400d6c0a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../datasets/diversevul_20230702_metadata.json\"\n",
    "data = []\n",
    "a = 0\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:  # Skip empty lines\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                a=a+1\n",
    "                print(f\"Skipping line due to JSONDecodeError: {e}\")\n",
    "\n",
    "metadata_dv = pd.DataFrame(data)\n",
    "\n",
    "def extract_cwe_number(cwe_list):\n",
    "    if len(cwe_list) > 0:\n",
    "        return int(cwe_list[0].split('-')[1])\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439a78ee-c04d-43f4-b1e6-d19eb0223222",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dv = df_diversevul.merge(metadata_dv[['commit_id', 'CVE', 'commit_url']], on='commit_id', how='left')\n",
    "df_dv = df_dv.drop(columns=['commit_id','hash','size','message','commit_url','project'])\n",
    "df_dv['year'] = df_dv['CVE'].str.extract(r'CVE-(\\d{4})')\n",
    "df_dv['year'] = df_dv['year'].fillna(0)\n",
    "df_dv['year'] = df_dv['year'].astype(int)\n",
    "df_dv = df_dv.drop(columns=['CVE'])\n",
    "df_dv['cwe_number'] = df_dv['cwe'].apply(lambda x: extract_cwe_number(x))\n",
    "df_dv['cwe_number'] = df_dv['cwe_number'].fillna(0).astype(int)\n",
    "df_dv = df_dv.drop(columns=['cwe'])\n",
    "df_dv = df_dv.rename(columns={'cwe_number': 'cwe','func':'text','target':'label'})\n",
    "df_dv['source'] = \"diversevul\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093b52f9-f4d9-43c3-8c80-4cd56fcf6899",
   "metadata": {},
   "source": [
    "# CrossVul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b099d44-bd11-4a19-822d-ca2086d17ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"../../datasets/dataset_final_sorted\"\n",
    "metadata_path = os.path.join(root_dir, \"metadata.json\")\n",
    "\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "database_name_to_year = {}\n",
    "for entry in metadata:\n",
    "    year = entry[\"cve\"].split(\"-\")[1]\n",
    "    for file_info in entry[\"files\"]:\n",
    "        database_name_to_year[file_info[\"database_name\"]] = year\n",
    "\n",
    "file_paths = []\n",
    "labels = []\n",
    "file_contents = []\n",
    "cwe_numbers = []\n",
    "years = []\n",
    "\n",
    "for root, dirs, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "        if \"c\" in root and (file.startswith(\"bad_\") or file.startswith(\"good_\")):\n",
    "            file_path = os.path.join(root, file)\n",
    "            label = 1 if file.startswith(\"bad_\") else 0\n",
    "            \n",
    "            with open(file_path, \"r\") as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            cwe_number = os.path.basename(os.path.dirname(os.path.dirname(file_path)))\n",
    "            if cwe_number.startswith(\"CWE-\"):\n",
    "                cwe_number = cwe_number[4:]\n",
    "            else:\n",
    "                cwe_number = \"None\"\n",
    "            \n",
    "            database_name = os.path.splitext(file)[0]  # Get the database name without extension\n",
    "            year = database_name_to_year.get(database_name, \"Unknown\")\n",
    "            \n",
    "            file_paths.append(file_path)\n",
    "            labels.append(label)\n",
    "            file_contents.append(content)\n",
    "            cwe_numbers.append(cwe_number)\n",
    "            years.append(year)\n",
    "\n",
    "df_cv = pd.DataFrame({\n",
    "    \"label\": labels,\n",
    "    \"text\": file_contents,\n",
    "    \"cwe\": cwe_numbers,\n",
    "    \"year\": years,\n",
    "    \"source\": \"crossvul\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82900e8-6dba-4a8a-9ecd-6a771ef87cb7",
   "metadata": {},
   "source": [
    "# CVEFixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa47332-dd11-4fce-b453-00fec8df89a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cvefixes_old = pd.read_json(\"../data/cvefixes_old.json\")\n",
    "df_cvefixes_22 = pd.read_json(\"../../datasets/cvefixes_data_22.json\")\n",
    "df_cvefixes_22['year'] = 2022\n",
    "df_cvefixes_23 = pd.read_json(\"../../datasets/cvefixes_data_23.json\")\n",
    "df_cvefixes_23['year'] = 2023\n",
    "df_cvefixes_24 = pd.read_json(\"../../datasets/cvefixes_data_24.json\")\n",
    "df_cvefixes_24['year'] = 2024\n",
    "df_cf = pd.concat([df_cvefixes_old, df_cvefixes_22, df_cvefixes_23, df_cvefixes_24], ignore_index=True)\n",
    "df_cf['target'] = (df_cf['before_change'] == 'True').astype(int)\n",
    "df_cf = df_cf.drop(columns=['name','signature','nloc','parameters','token_count','programming_language','repo_name','published_date','before_change'])\n",
    "df_cf['cwe_id'] = df_cf['cwe_id'].str.extract(r'CWE-(\\d+)')\n",
    "df_cf['cwe_id'] = pd.to_numeric(df_cf['cwe_id'], errors='coerce')\n",
    "df_cf = df_cf.dropna(subset=['cwe_id'])\n",
    "df_cf['cwe_id'] = df_cf['cwe_id'].astype(int)\n",
    "df_cf = df_cf.reset_index(drop=True)\n",
    "df_cf = df_cf.rename(columns={'cwe_id': 'cwe','code':'text','target':'label'})\n",
    "df_cf['source'] = \"cvefixes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce96b305",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f19368b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_dv, df_cv, df_cf], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32d574f",
   "metadata": {},
   "source": [
    "# Remove duplicates and take < 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6353ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original merged df length:  362758\n",
      "Non duplicated df length: 340502\n",
      "Duplicate entries:  22256\n"
     ]
    }
   ],
   "source": [
    "df['text_clean'] = df['text'].str.replace(r'[\\s\\n\\t\\r]', '', regex=True)\n",
    "\n",
    "# Compute MD5 hash of the normalized text\n",
    "df['md5_hash'] = df['text_clean'].apply(lambda x: hashlib.md5(x.encode('utf-8')).hexdigest())\n",
    "\n",
    "# De-duplicate based on MD5 hash\n",
    "nondup_df = df.drop_duplicates(subset=['md5_hash'], ignore_index=True)\n",
    "\n",
    "# Drop temporary columns\n",
    "nondup_df = nondup_df.drop(['text_clean'], axis=1)\n",
    "nondup_df = nondup_df.rename(columns={\"md5_hash\":\"hash\"})\n",
    "df = df.drop(['text_clean', 'md5_hash'], axis=1)\n",
    "\n",
    "# Output results\n",
    "print(\"Original merged df length: \", len(df))\n",
    "print(\"Non duplicated df length:\", len(nondup_df))\n",
    "print(\"Duplicate entries: \", len(df) - len(nondup_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8db9c25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non duplicated df length (all): 340502\n",
      "filtered non-duplicated df length (< 4000): 312542\n"
     ]
    }
   ],
   "source": [
    "print(\"non duplicated df length (all):\", len(nondup_df))\n",
    "lengths = nondup_df['text'].apply(len)\n",
    "nondup_df = nondup_df[lengths < 4000]\n",
    "print(\"filtered non-duplicated df length (< 4000):\", len(nondup_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9a4547",
   "metadata": {},
   "source": [
    "# Sort by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de4af283",
   "metadata": {},
   "outputs": [],
   "source": [
    "nondup_df['year'] = pd.to_numeric(nondup_df['year'], errors='coerce')\n",
    "nondup_df['year'] = nondup_df['year'].fillna(0).astype(int)\n",
    "nondup_df.loc[nondup_df['label'] == 0, 'cwe'] = 0\n",
    "nondup_df = nondup_df.sort_values(by='year')\n",
    "nondup_df['cwe'].replace('None', 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4df87387",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = nondup_df[nondup_df['label'] == 1]\n",
    "df_0 = nondup_df[nondup_df['label'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84528513",
   "metadata": {},
   "source": [
    "# Take top 5 CWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "00ec8ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwe_counts = df_1['cwe'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e995aeda",
   "metadata": {},
   "source": [
    "cwe_counts.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "23fe5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_random_entries(df, n):\n",
    "    sampled_df = df_0.sample(n, random_state=42)\n",
    "    df_remaining = df.drop(sampled_df.index)\n",
    "    return sampled_df, df_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f78e5571",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_125 = nondup_df[nondup_df['cwe'] == 125]\n",
    "df_1_787 = nondup_df[nondup_df['cwe'] == 787]\n",
    "df_1_119 = nondup_df[nondup_df['cwe'] == 119]\n",
    "df_1_20 = nondup_df[nondup_df['cwe'] == 20]\n",
    "df_1_416 = nondup_df[nondup_df['cwe'] == 416]\n",
    "df_1 = df_1[~df_1['cwe'].isin([125,787,119,20,416])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1069d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_125, df_0 = take_random_entries(df_0, len(df_1_125))\n",
    "df_0_787, df_0 = take_random_entries(df_0, len(df_1_787))\n",
    "df_0_119, df_0 = take_random_entries(df_0, len(df_1_119))\n",
    "df_0_20, df_0 = take_random_entries(df_0, len(df_1_20))\n",
    "df_0_416, df_0 = take_random_entries(df_0, len(df_1_416))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c0cbe83",
   "metadata": {},
   "source": [
    "df_1_125 = df_1_125.sort_values(by='year')\n",
    "train_1_125 = df_1_125.iloc[:int(len(df_1_125) * 0.9)]\n",
    "test_1_125 = df_1_125.iloc[int(len(df_1_125) * 0.9):]\n",
    "train_0_125, test_0_125 = train_test_split(df_0_125, test_size=0.1, random_state=42)\n",
    "train_125 = pd.concat([train_1_125, train_0_125], ignore_index=True)\n",
    "test_125 = pd.concat([test_1_125, test_0_125], ignore_index=True)\n",
    "df_0 = pd.concat([df_0, train_0_125], ignore_index=True)\n",
    "df_1 = pd.concat([df_1, train_1_125], ignore_index=True)\n",
    "train_125.to_json(\"train_125.json\")\n",
    "test_125.to_json(\"test_125.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "578d837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1_125, test_1_125 = train_test_split(df_1_125, test_size=0.1, random_state=42)\n",
    "train_0_125, test_0_125 = train_test_split(df_0_125, test_size=0.1, random_state=42)\n",
    "train_125 = pd.concat([train_1_125, train_0_125], ignore_index=True)\n",
    "test_125 = pd.concat([test_1_125, test_0_125], ignore_index=True)\n",
    "df_0 = pd.concat([df_0, train_0_125], ignore_index=True)\n",
    "df_1 = pd.concat([df_1, train_1_125], ignore_index=True)\n",
    "train_125.to_json(\"train_125.json\")\n",
    "test_125.to_json(\"test_125.json\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4ce1dee",
   "metadata": {},
   "source": [
    "df_1_787 = df_1_787.sort_values(by='year')\n",
    "train_1_787 = df_1_787.iloc[:int(len(df_1_787) * 0.9)]\n",
    "test_1_787 = df_1_787.iloc[int(len(df_1_787) * 0.9):]\n",
    "train_0_787, test_0_787 = train_test_split(df_0_787, test_size=0.1, random_state=42)\n",
    "train_787 = pd.concat([train_1_787, train_0_787], ignore_index=True)\n",
    "test_787 = pd.concat([test_1_787, test_0_787], ignore_index=True)\n",
    "df_0 = pd.concat([df_0, train_0_787], ignore_index=True)\n",
    "df_1 = pd.concat([df_1, train_1_787], ignore_index=True)\n",
    "train_787.to_json(\"train_787.json\")\n",
    "test_787.to_json(\"test_787.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "31d91429",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1_787, test_1_787 = train_test_split(df_1_787, test_size=0.1, random_state=42)\n",
    "train_0_787, test_0_787 = train_test_split(df_0_787, test_size=0.1, random_state=42)\n",
    "train_787 = pd.concat([train_1_787, train_0_787], ignore_index=True)\n",
    "test_787 = pd.concat([test_1_787, test_0_787], ignore_index=True)\n",
    "df_0 = pd.concat([df_0, train_0_787], ignore_index=True)\n",
    "df_1 = pd.concat([df_1, train_1_787], ignore_index=True)\n",
    "train_787.to_json(\"train_787.json\")\n",
    "test_787.to_json(\"test_787.json\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19bd47fe",
   "metadata": {},
   "source": [
    "df_1_119 = df_1_119.sort_values(by='year')\n",
    "train_1_119 = df_1_119.iloc[:int(len(df_1_119) * 0.9)]\n",
    "test_1_119 = df_1_119.iloc[int(len(df_1_119) * 0.9):]\n",
    "train_0_119, test_0_119 = train_test_split(df_0_119, test_size=0.1, random_state=42)\n",
    "train_119 = pd.concat([train_1_119, train_0_119], ignore_index=True)\n",
    "test_119 = pd.concat([test_1_119, test_0_119], ignore_index=True)\n",
    "df_0 = pd.concat([df_0, train_0_119], ignore_index=True)\n",
    "df_1 = pd.concat([df_1, train_1_119], ignore_index=True)\n",
    "train_119.to_json(\"train_119.json\")\n",
    "test_119.to_json(\"test_119.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e1e6b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1_119, test_1_119 = train_test_split(df_1_119, test_size=0.1, random_state=42)\n",
    "train_0_119, test_0_119 = train_test_split(df_0_119, test_size=0.1, random_state=42)\n",
    "train_119 = pd.concat([train_1_119, train_0_119], ignore_index=True)\n",
    "test_119 = pd.concat([test_1_119, test_0_119], ignore_index=True)\n",
    "df_0 = pd.concat([df_0, train_0_119], ignore_index=True)\n",
    "df_1 = pd.concat([df_1, train_1_119], ignore_index=True)\n",
    "train_119.to_json(\"train_119.json\")\n",
    "test_119.to_json(\"test_119.json\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f48e1a2e",
   "metadata": {},
   "source": [
    "df_1_20 = df_1_20.sort_values(by='year')\n",
    "train_1_20 = df_1_20.iloc[:int(len(df_1_20) * 0.9)]\n",
    "test_1_20 = df_1_20.iloc[int(len(df_1_20) * 0.9):]\n",
    "train_0_20, test_0_20 = train_test_split(df_0_20, test_size=0.1, random_state=42)\n",
    "train_20 = pd.concat([train_1_20, train_0_20], ignore_index=True)\n",
    "test_20 = pd.concat([test_1_20, test_0_20], ignore_index=True)\n",
    "df_0 = pd.concat([df_0, train_0_20], ignore_index=True)\n",
    "df_1 = pd.concat([df_1, train_1_20], ignore_index=True)\n",
    "train_20.to_json(\"train_20.json\")\n",
    "test_20.to_json(\"test_20.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "025107d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1_20, test_1_20 = train_test_split(df_1_20, test_size=0.1, random_state=42)\n",
    "train_0_20, test_0_20 = train_test_split(df_0_20, test_size=0.1, random_state=42)\n",
    "train_20 = pd.concat([train_1_20, train_0_20], ignore_index=True)\n",
    "test_20 = pd.concat([test_1_20, test_0_20], ignore_index=True)\n",
    "df_0 = pd.concat([df_0, train_0_20], ignore_index=True)\n",
    "df_1 = pd.concat([df_1, train_1_20], ignore_index=True)\n",
    "train_20.to_json(\"train_20.json\")\n",
    "test_20.to_json(\"test_20.json\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dfe3a690",
   "metadata": {},
   "source": [
    "df_1_416 = df_1_416.sort_values(by='year')\n",
    "train_1_416 = df_1_416.iloc[:int(len(df_1_416) * 0.9)]\n",
    "test_1_416 = df_1_416.iloc[int(len(df_1_416) * 0.9):]\n",
    "train_0_416, test_0_416 = train_test_split(df_0_416, test_size=0.1, random_state=42)\n",
    "train_416 = pd.concat([train_1_416, train_0_416], ignore_index=True)\n",
    "test_416 = pd.concat([test_1_416, test_0_416], ignore_index=True)\n",
    "df_0 = pd.concat([df_0, train_0_416], ignore_index=True)\n",
    "df_1 = pd.concat([df_1, train_1_416], ignore_index=True)\n",
    "train_416.to_json(\"train_416.json\")\n",
    "test_416.to_json(\"test_416.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "99165b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1_416, test_1_416 = train_test_split(df_1_416, test_size=0.1, random_state=42)\n",
    "train_0_416, test_0_416 = train_test_split(df_0_416, test_size=0.1, random_state=42)\n",
    "train_416 = pd.concat([train_1_416, train_0_416], ignore_index=True)\n",
    "test_416 = pd.concat([test_1_416, test_0_416], ignore_index=True)\n",
    "df_0 = pd.concat([df_0, train_0_416], ignore_index=True)\n",
    "df_1 = pd.concat([df_1, train_1_416], ignore_index=True)\n",
    "train_416.to_json(\"train_416.json\")\n",
    "test_416.to_json(\"test_416.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cc166770",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_0, df_1], ignore_index=True)\n",
    "df.to_json(\"train_all.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aff8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
